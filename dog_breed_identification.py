# -*- coding: utf-8 -*-
"""Dog breed identification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k3AlMAPpfJEM3MrQB_5w8I1QCmml0uGe

# Doggo breed predictooooor üê∂

## 1. Problem

Classify the dog breed from an image when sitting in the park taking a picture

## 2. Data

The data used to train the model is fetched from kaggle

https://www.kaggle.com/c/dog-breed-identification/data?select=test

## 3. Evaluation

The models performance is evaluated by a multi class log loss where the probability of each dog breed is given from the possible breeds in the dataset

## 4. Features

Some information on the dataset:
* We are dealing with unstructured data (images). So deep learning/transfer learning is the perfered method to solve the problem
* There are 120 different breeds of dogs. i.e. 120 classes in the dataset
* There are around 10,000+ images in the train set (with labels) and 10,000+ images in the test set (without labels)
"""

#!unzip "/content/drive/MyDrive/Dog vision/dog-breed-identification.zip" -d "/content/drive/MyDrive/Dog vision/data/"

import tensorflow as tf
import tensorflow_hub as hub
print("Version tf: ", tf.__version__)
print("Version hub: ", hub.__version__)

# check gpu availablity
print("GPU available!!" if tf.config.list_physical_devices("GPU")  else "GPU not available")

"""## Getting the data ready üìà"""

# checking out the labels
import pandas as pd
labels_csv = pd.read_csv("/content/drive/MyDrive/Dog vision/data/labels.csv")
labels_csv.describe()
labels_csv.head()

# How many images are there per breed?
labels_csv["breed"].value_counts().plot.bar(figsize=(20,10))

# It is optimal to have 100 datapoints per label, minimum 10
labels_csv["breed"].value_counts().median()

# Let¬¥s view an image
from IPython.display import Image

# Display the image using the Image class
display(Image(filename="/content/drive/MyDrive/Dog vision/data/train/0021f9ceb3235effd7fcde7f7538ed62.jpg"))

"""### Getting images and their labels

Let¬¥s get a list of the images and the labels
"""

filenames = ["/content/drive/MyDrive/Dog vision/data/train/" + fname + ".jpg" for fname in labels_csv["id"]]

# get the first 10
filenames[:10]

import numpy as np
labels = labels_csv["breed"].to_numpy()
unique_breeds = np.unique(labels)
unique_breeds

# boolean labels
boolean_labels = [label == unique_breeds for label in labels]
boolean_labels[:2]

"""### Create our own validation set


"""

# Set up X and y variables

X = filenames
y = boolean_labels

NUM_IMAGES = 1000 # @param {type: "slider", min: 1000, max: 10000, step: 1000}

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES], y[:NUM_IMAGES], test_size=0.2, random_state=42)
len(X_train), len(y_train), len(X_val), len(y_val)

X_train[:5], y_train[:2]

"""### Preprocess Images

Turn images into tensors
"""

# convert image to numpy array
from matplotlib.pyplot import imread
image = imread(filenames[42])
image.shape

# min and max values of the rgb values in the image
image.max(), image.min()

image[:2]

# turn image numpy array into tensor
tf.constant(image[:2])

# turn all images into tensors with rgb data

IMAGE_SIZE = 224

def preprocess_image(image_path):
  """
  Takes image file path and turns it into a tensor
  """
  # read image file
  image = tf.io.read_file(image_path)
  # turn jpeg into numerical values
  image = tf.image.decode_jpeg(image, channels=3)
  # normalize the values 0-255 to 0-1
  image = tf.image.convert_image_dtype(image, tf.float32)
  # rezise the image to (224, 224)
  image = tf.image.resize(image, size=[IMAGE_SIZE, IMAGE_SIZE])

  return image

# Turning data into batches. We need the data in the format of `(images, labels)`. Then the images need to be batched in sizes of 32.

def get_image_label(image_path, label):
  image = preprocess_image(image_path)
  return image, label

BATCH_SIZE = 32

def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  # shuffle data if training data, not shuffle if validation data

  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths, no labels
    data_batch = data.map(preprocess_image).batch(batch_size)
    return data_batch

  if valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) # filepaths and labels
    data_batch = data.map(get_image_label).batch(batch_size)
    return data_batch

  else:
    print("Creating training data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) # filepaths and labels
    # shuffle pathnames and labels
    data = data.shuffle(buffer_size=len(X))

    data = data.map(get_image_label)
    data_batch = data.batch(batch_size)

  return data_batch

# Create training and validation data

train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val, y_val, valid_data=True)

train_data.element_spec, val_data.element_spec

"""### Visualize our data batches

"""

import matplotlib.pyplot as plt

def show_25_images(images, labels):
  plt.figure(figsize=(12, 12))
  for i in range(25):
    ax = plt.subplot(5, 5, i+1)
    plt.imshow(images[i])
    # image label as title
    plt.title(unique_breeds[labels[i].argmax()])
    # turn off grid lines
    plt.axis("off")

# fetch first batch from batched training data and visualize
train_images, train_labels = next(train_data.as_numpy_iterator())
show_25_images(train_images, train_labels)

# fetch first batch from batched training data and visualize
val_images, val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

"""### Building a model

* define input in terms of tensor shape
* define outup in terms of tensor shape
* define the model in terms of URL to the model
"""

IMAGE_SIZE

INPUT_SHAPE = [None, IMAGE_SIZE, IMAGE_SIZE, 3]

OUTPUT_SHAPE = len(unique_breeds)

MODEL_URL = "https://www.kaggle.com/models/google/mobilenet-v2/TensorFlow2/130-224-classification/2"

def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print(f"Building a model with {model_url}")
  model = tf.keras.Sequential([
    hub.KerasLayer(model_url), # layer 1 pretrained model
    tf.keras.layers.Dense(units=output_shape, activation="softmax") # layer 2 dense NN, untrained
  ])
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"]
  )
  model.build(input_shape)
  return model

model = create_model()
model.summary()

"""### Setup callbacks"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import datetime
import os

def create_tensorboard_callback():
  logdir = os.path.join("/content/drive/MyDrive/Dog vision/logs", datetime.datetime.now().strftime("Y%m%d-H%M%s"))
  return tf.keras.callbacks.TensorBoard(logdir)

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy", patience=3)

"""## Train our model (on subset of data)

train on subset to test before we use the entire dataset (1000 images)
"""

NUM_EPOCHS = 100 # @param {type: "slider", min:10, max:100, step:10}

# check to make sure we are running on a GPU
print("GPU available!!" if tf.config.list_physical_devices("GPU")  else "GPU not available")

def train_model():
  model = create_model()

  tensorboard = create_tensorboard_callback()

  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, early_stopping])

  return model

model = train_model()

"""### Tensorboard view"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/Dog\ vision/logs

"""## Making predictions and evaluating the model"""

predictions = model.predict(val_data, verbose=1)
predictions

def get_pred_label(prediction_probabilities):
  return unique_breeds[np.argmax(prediction_probabilities)]

pred_label = get_pred_label(predictions[0])

def unbatchify(data):
  images = []
  labels = []
  for image, label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])
  return images, labels

val_images, val_labels = unbatchify(val_data)

"""### Visualize the prediction"""

def plot_pred(prediction_probabilities, labels, images, n=1):
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]
  pred_label = get_pred_label(pred_prob)

  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  plt.title("predicted: {}. Confidence: {:2.0f}%. Actual breed: {}".format(pred_label, np.max(pred_prob)*100, true_label), color=color)

plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=42)

#for n in range(len(val_images)):
#  plot_pred(prediction_probabilities=predictions, val_labels, val_images, n=n)

"""### Visualze top 10 predictions by confidence"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  pred_prob, true_label = prediction_probabilities[n], labels[n]
  pred_label = get_pred_label(pred_prob)

  top_10_pred_indexes = list(pred_prob.argsort()[-10:][::-1])
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  top_10_pred_labels = np.array(labels)[top_10_pred_indexes]


  # plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                        top_10_pred_values,
                        color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)), top_10_pred_labels, rotation="vertical")

  # change color if correct prediction
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass


plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, n=9)

"""### Visualize for evaluation"""

i_multitplier = 0 # where in the dataset to start plot the next 6 images in a grid of 3*2
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=i+i_multitplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, n=i+i_multitplier)

plt.tight_layout(h_pad=1.0)
plt.show()

"""#### challenge: create a confusion matrix for seeing where the model gets confused

### Saving and loading the trained model
"""

def save_model(model, suffix=None):
  modeldir = os.path.join("/content/drive/MyDrive/Dog vision/models", datetime.datetime.now().strftime("Y%m%d-H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5"  # save format of model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

def load_model(model_path):
  model = tf.keras.models.load_model(model_path, custom_objects={"KerasLayer": hub.KerasLayer})
  return model

# save model

model_1000_images_saved_path = save_model(model, suffix="1000-images-mobilenetv2-Adam")

# load model

loaded_1000_images_model = load_model(model_1000_images_saved_path)

# evaluate mode pre saving

model.evaluate(val_data)

# evaluate mode after saving

loaded_1000_images_model.evaluate(val_data)

"""## Train the big dog model on full dataset üê∂"""

# length of original dataset

len(X), len(y)

# Create a data batch with full data set

full_data = create_data_batches(X, y)
full_data

full_model = create_model()

# Create full model callbacks

full_model_tensorboard = create_tensorboard_callback()

# No validation set when training on full data, so we can¬¥t monitor validation accuracy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy", patience=3)

"""**Note:** Running this cell below might take 30 minutes for the first epoch"""

# Train full model
full_model.fit(x=full_data, epochs=NUM_EPOCHS, callbacks=[full_model_tensorboard, full_model_early_stopping])

# Save the trained model
model_id_suffix = "full-image-set-mobilenetv2-Adam"
full_model_saved_path = save_model(full_model, suffix=model_id_suffix)

# Load the trained model
full_model_saved_path = "/content/drive/MyDrive/Dog vision/models/Y0524-H311716586278-1000-images-mobilenetv2-Adam.h5"
loaded_full_model = load_model(full_model_saved_path)

"""### Make predictions on the test data set"""

# get test data filenames
import os
test_path = "/content/drive/MyDrive/Dog vision/data/test/"
test_filenames = [test_path + fname for fname in os.listdir(test_path)]

# create batches of test data in correct format from filenames
test_data = create_data_batches(test_filenames, test_data=True)

# run predictions
test_predictions = loaded_full_model.predict(test_data, verbose=1)

# save predictions
test_predictions_saved_path = f"/content/drive/MyDrive/Dog vision/predictions/{model_id_suffix}.csv"
np.savetxt(test_predictions_saved_path, test_predictions, delimiter=",")

# load predictions from file
loaded_test_predictions = np.loadtxt(test_predictions_saved_path, delimiter=",")
loaded_test_predictions

"""### Convert predictions to kaggle format for submission

Format should like like this in a csv:
```
id,affenpinscher,afghan_hound,..,yorkshire_terrier
000621fb3cbb32d8935728e48679680e,0.0083,0.0,...,0.0083
etc.
```
"""

# create dataframe headers
preds_df = pd.DataFrame(columns=["id"] + list(unique_breeds))

# append image ids to the id column
test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
preds_df["id"] = test_ids

# append test predictions for each image to each breed column
preds_df[list(unique_breeds)] = test_predictions

# save df as csv
submission_filename = "full_model_predictions_submission_1_mobilenetv2.csv"
preds_df.to_csv("/content/drive/MyDrive/Dog vision/kaggle-submissions/" + submission_filename, index=False)

"""## Making predctions on custom images"""

custom_path = "/content/drive/MyDrive/Dog vision/my-dog-photos/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path)]

# turn custom images into batch dataset
custom_data = create_data_batches(custom_image_paths, test_data=True)

# make predictions on custom data using the trained full model
custom_preds = loaded_full_model.predict(custom_data)

custom_preds.shape

custom_pred_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_labels

custom_images = []
for image in custom_data.unbatch():
    custom_images.append(image.numpy())

# plot the predictions. Need to fix method above to also be able to plot without labels
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 10))
for i, image in enumerate(custom_images):
  plt.subplot(1, len(custom_images), i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_labels[i])
  plt.imshow(image)